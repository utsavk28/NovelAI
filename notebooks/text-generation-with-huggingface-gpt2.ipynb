{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers -qq","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:23:47.401173Z","iopub.execute_input":"2022-02-12T12:23:47.401710Z","iopub.status.idle":"2022-02-12T12:23:54.696484Z","shell.execute_reply.started":"2022-02-12T12:23:47.401594Z","shell.execute_reply":"2022-02-12T12:23:54.695548Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#for reproducability\nSEED = 34\n\n#maximum number of words in output text\nMAX_LEN = 70\n\ninput_sequence = \"I don't know about you, but there's only one thing I want to do after a long day of work\"\n\n#get transformers\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n\n#get large GPT2 tokenizer and GPT2 model\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\nGPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n\n#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n\n#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n\n#view model parameters\nGPT2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:23:54.717384Z","iopub.execute_input":"2022-02-12T12:23:54.717783Z","iopub.status.idle":"2022-02-12T12:25:31.189402Z","shell.execute_reply.started":"2022-02-12T12:23:54.717743Z","shell.execute_reply":"2022-02-12T12:25:31.188637Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# II. Different Decoding Methods\n\n## First Pass (Greedy Search)\n\n**With Greedy search, the word with the highest probability is predicted as the next word i.e. the next word is updated via:**\n\n$$w_t = argmax_{w}P(w | w_{1:t-1})$$\n\n**at each timestep $t$. Let's see how this naive approach performs:**","metadata":{}},{"cell_type":"code","source":"#get deep learning basics\nimport tensorflow as tf\ntf.random.set_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:25:31.192366Z","iopub.execute_input":"2022-02-12T12:25:31.192689Z","iopub.status.idle":"2022-02-12T12:25:31.197582Z","shell.execute_reply.started":"2022-02-12T12:25:31.192656Z","shell.execute_reply":"2022-02-12T12:25:31.196670Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# encode context the generation is conditioned on\ninput_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n\n# generate text until the output length (which includes the context length) reaches 50\ngreedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(greedy_output[0], skip_special_tokens = True))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:25:31.199361Z","iopub.execute_input":"2022-02-12T12:25:31.199988Z","iopub.status.idle":"2022-02-12T12:25:42.701408Z","shell.execute_reply.started":"2022-02-12T12:25:31.199943Z","shell.execute_reply":"2022-02-12T12:25:42.699906Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**And there we go: generating text is that easy. Our results are not great - as we can see, our model starts repeating itself rather quickly. The main issue with Greedy Search is that words with high probabilities can be masked by words in front of them with low probabilities, so the model is unable to explore more diverse combinations of words. We can prevent this by implementing Beam Search:**","metadata":{}},{"cell_type":"markdown","source":"## Beam Search with N-Gram Penalities\n\n**Beam search is essentially Greedy Search but the model tracks and keeps `num_beams` of hypotheses at each time step, so the model is able to compare alternative paths as it generates text. We can also include a n-gram penalty by setting `no_repeat_ngram_size = 2` which ensures that no 2-grams appear twice. We will also set `num_return_sequences = 5` so we can see what the other 5 beams looked like**\n\n**To use Beam Search, we need only modify some parameters in the `generate` function:**","metadata":{}},{"cell_type":"code","source":"# set return_num_sequences > 1\nbeam_outputs = GPT2.generate(\n    input_ids, \n    max_length = MAX_LEN, \n    num_beams = 5, \n    no_repeat_ngram_size = 2, \n    num_return_sequences = 5, \n    early_stopping = True\n)\n\nprint('')\nprint(\"Output:\\n\" + 100 * '-')\n\n# now we have 3 output sequences\nfor i, beam_output in enumerate(beam_outputs):\n      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:25:42.702819Z","iopub.execute_input":"2022-02-12T12:25:42.703191Z","iopub.status.idle":"2022-02-12T12:25:59.343918Z","shell.execute_reply.started":"2022-02-12T12:25:42.703149Z","shell.execute_reply":"2022-02-12T12:25:59.343174Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Now that's much better! The 5 different beam hypotheses are pretty much all the same, but if we increaed `num_beams`, then we would see some more variation in the separate beams. But of course, Beam Search is not perfect either. It works well when the legnth of the generated text is more or less constant, like problems in translation or summarization, but not so much for open-ended problems like dialog or story generation (because it is much harder to find a balance between `num_beams` and `no_repeat_ngram_size`)**\n\n**Furthermore, [research](https://arxiv.org/abs/1904.09751) shows that human languages do not follow this 'high probability word next' distribution. This makes sense - if my words were exactly what you expected them to be, I would be quite a boring person and most people don't want to be boring! The below graph plots the difference of Beam Search and actual human speech: ![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)**\n\nTaken from original paper [here](https://arxiv.org/abs/1904.09751)","metadata":{}},{"cell_type":"markdown","source":"## Basic Sampling\n\n**Now we will explore indeterministic decodings - sampling. Instead of following a strict path to find the end text with the highest probability, we instead randomly pick the next word by its conditional probability distribution:**\n\n$$w_t \\sim P(w|w_{1:t-1})$$\n\n**However, when we include this randomness, the generated text tends to be incoherent (see more [here](https://arxiv.org/pdf/1904.09751.pdf)) so we can include the `temperature` parameter which increases the chances of high probability words and decreases the chances of low probability words in the sampling:**\n\n**We just need to set `do_sample = True` to implement sampling and for demonstration purposes (you'll shortly see why) we set `top_k = 0`:**","metadata":{}},{"cell_type":"code","source":"# use temperature to decrease the sensitivity to low probability candidates\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_k = 0, \n                             temperature = 0.8\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:25:59.345237Z","iopub.execute_input":"2022-02-12T12:25:59.345610Z","iopub.status.idle":"2022-02-12T12:26:10.637701Z","shell.execute_reply.started":"2022-02-12T12:25:59.345577Z","shell.execute_reply":"2022-02-12T12:26:10.636774Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Top-K Sampling\n\n**In Top-K sampling, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high probability words occuring and decreasing the chances of low probabillity words, we just remove low probability words all together**\n\n**We just need to set `top_k` to however many of the top words we want to consider for our conditional probability distribution:**","metadata":{}},{"cell_type":"code","source":"#sample from only top_k most likely words\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_k = 50\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:26:10.639026Z","iopub.execute_input":"2022-02-12T12:26:10.639565Z","iopub.status.idle":"2022-02-12T12:26:22.331565Z","shell.execute_reply.started":"2022-02-12T12:26:10.639522Z","shell.execute_reply":"2022-02-12T12:26:22.330651Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Top-K Sampling seems to generate more coherent text than our random sampling before. But we can do even better:**","metadata":{}},{"cell_type":"markdown","source":"## Top-P Sampling\n\n**Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely wordsm we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set**\n\n**The main difference here is that with Top-K sampling, the size of the set of words is static (obviously) whereas in Top-P sampling, the size of the set can change. To use this sampling method, we just set `top_k = 0` and choose a value `top_p`:**","metadata":{}},{"cell_type":"code","source":"#sample only from 80% most likely words\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_p = 0.8, \n                             top_k = 0\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:26:22.332875Z","iopub.execute_input":"2022-02-12T12:26:22.333416Z","iopub.status.idle":"2022-02-12T12:26:34.528220Z","shell.execute_reply.started":"2022-02-12T12:26:22.333373Z","shell.execute_reply":"2022-02-12T12:26:34.526842Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Top-K and Top-P Sampling\n\n**As you could have probably guessed, we can use both Top-K and Top-P sampling here. This reduces the chances of us getting weird words (low probability words) while allowing for a dynamic selection size. We need only top a value for both `top_k` and `top_p`. We can even include the inital temperature parameter if we want to, Let's now see how our model performs now after adding everything together. We will check the top 5 return to see how diverse our answers are:**","metadata":{}},{"cell_type":"code","source":"#combine both sampling techniques\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .7,\n                              top_k = 50, \n                              top_p = 0.85, \n                              num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:26:49.129737Z","iopub.execute_input":"2022-02-12T12:26:49.130059Z","iopub.status.idle":"2022-02-12T12:27:19.656941Z","shell.execute_reply.started":"2022-02-12T12:26:49.130029Z","shell.execute_reply":"2022-02-12T12:27:19.656181Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 150\nprompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\ninput_ids = tokenizer.encode(prompt1, return_tensors='tf')\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85 \n                              #num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')\n    \nprint(\"=\"*200)\n    \nprompt2 = 'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.'\ninput_ids = tokenizer.encode(prompt2, return_tensors='tf')\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85\n                              #num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')\n    \nprint(\"=\"*200)\n    \nprompt3 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.'\n\ninput_ids = tokenizer.encode(prompt3, return_tensors='tf')\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85 \n                              #num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')\n    \nprint(\"=\"*200)\n    \nprompt4 = \"For today’s homework assignment, please describe the reasons for the US Civil War.\"\ninput_ids = tokenizer.encode(prompt4, return_tensors='tf')\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85 \n                              #num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:27:19.658798Z","iopub.execute_input":"2022-02-12T12:27:19.659173Z","iopub.status.idle":"2022-02-12T12:28:59.997353Z","shell.execute_reply.started":"2022-02-12T12:27:19.659134Z","shell.execute_reply":"2022-02-12T12:28:59.996398Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Trying Light Novel Statements","metadata":{}},{"cell_type":"code","source":"import random \n\ndef generate_next_words(ip_text,length) :\n    input_ids = tokenizer.encode(ip_text, return_tensors='tf')\n    sample_outputs = GPT2.generate(\n                                  input_ids,\n                                  do_sample = True, \n                                  max_length = length,                              \n                                  #temperature = .8,\n                                  top_k = 50, \n                                  top_p = 0.85 \n                                  #num_return_sequences = 5\n    )\n    return tokenizer.decode(random.choice(sample_outputs),skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:38:38.158672Z","iopub.execute_input":"2022-02-12T12:38:38.159017Z","iopub.status.idle":"2022-02-12T12:38:38.165265Z","shell.execute_reply.started":"2022-02-12T12:38:38.158987Z","shell.execute_reply":"2022-02-12T12:38:38.164261Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\n\nbase_path = \"../input/4-light-novel-for-text-generation\"\nlight_novel_path = \"../input/4-light-novel-for-text-generation/Level 999 Villager\"\nfilename_path = \"../input/4-light-novel-for-text-generation/Level 999 Villager/LV999-Villager-Volume-1.txt\"\n\nln_arr = None\n\nwith open(filename_path, 'r') as text:\n    textfile = text.readlines()\n#     print(textfile)\n    ln_arr = textfile\nprint(ln_arr[:10])\nln_arr = list(filter(lambda x:x!=\"\\n\", ln_arr))\nrand_sent = random.choice(ln_arr)\nrand_sent","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:39:07.239876Z","iopub.execute_input":"2022-02-12T12:39:07.240226Z","iopub.status.idle":"2022-02-12T12:39:07.253982Z","shell.execute_reply.started":"2022-02-12T12:39:07.240193Z","shell.execute_reply":"2022-02-12T12:39:07.252988Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"for sent in random.choices(ln_arr,k=2) :\n    print(generate_next_words(sent,100))\n    print(\"~\"*200)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:43:15.508624Z","iopub.execute_input":"2022-02-12T12:43:15.508958Z","iopub.status.idle":"2022-02-12T12:43:48.887396Z","shell.execute_reply.started":"2022-02-12T12:43:15.508925Z","shell.execute_reply":"2022-02-12T12:43:48.885793Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}